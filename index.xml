<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Advance&#39;s blog</title>
    <link>https://Advance2020.github.io/</link>
    <description>Recent content on Advance&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>cn</language>
    <lastBuildDate>Fri, 13 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://Advance2020.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Robust-Wide论文梳理</title>
      <link>https://Advance2020.github.io/posts/study/robust-wide%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/</link>
      <pubDate>Fri, 13 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://Advance2020.github.io/posts/study/robust-wide%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/</guid>
      <description>Robust-Wide: Robust Watermarking against Instruction-driven Image Editing 笔记 一、文章简介 论文链接：Robust-Wide: Robust Watermarking against Instruction-driven Image Editing
文章来源： ECCV 文章年份： 2024
核心思路： 引入一种新颖的部分指令驱动的去噪采样指导（PIDSG）模块，该模块由多种指令注入和不同语义级别的图像的大量修改组成。通过 PIDSG，编码器倾向于将水印嵌入到更鲁棒和语义感知的区域中，即使在严格的图像编辑之后，水印仍然存在，从而实现针对语义失真的鲁棒性。
二、背景知识 2.1 扩散模型 经典DDPM的目标函数： $$L_{DM}=E_{x,\epsilon\sim\mathcal{N}(0,1),t}\bigg[|\epsilon-\epsilon_\theta(x_t,t)|_2^2\bigg]$$
其中，$x$是输入图像，$\epsilon$是随机采样的高斯噪声，$t$是均匀采样的时间步长，$x_t$是$x$在$t$步时的加噪版本，$\epsilon_{\theta}$是扩散模型中用于预测噪声的神经网络。文生图模型如GLIDE、DALL·E2 和 Imagen都是基于 DDPM 的。
潜在空间LDM的目标函数： $$L_{LDM}=E_{\mathcal{E}(x),\epsilon\sim\mathcal{N}(0,1),t}\Big[|\epsilon-\epsilon_\theta(z_t,t)|_2^2\Big]$$
LDM在预训练的 VAE 的潜在空间中进行加噪和去噪处理，其中$\mathcal{E}$是VAE的编码器，$z_t$是噪声潜变量。SD是基于LDM的。
2.2 基于扩散模型的指令驱动的图像编辑 （1） InstructPix2Pix
InstructPix2Pix在前向过程直接进行图像编辑，无需额外样例图、对输入/输出图的描述或逐样本finetune。它以端到端的方式进行训练，数据集由 GPT-3 和 Prompt2Prompt 生成，数据集中的每个项目都包含 源图像、编辑指令和后续编辑的图像。 在训练过程中，图像和指令分别被视为条件 $c_I$ 和 $c_T$ ，而编辑后的图像是真实输出。因此，训练目标如下: $$L=E_{\mathcal{E}(x),\mathcal{E}(c_I),c_T,\epsilon\sim\mathcal{N}(0,1),t}\bigg[|\epsilon-\epsilon_\theta(z_t,t,\mathcal{E}(c_I),c_T)|_2^2\bigg]$$ （2）ControlNet-InstructPix2Pix
ControlNet是一个控制预训练图像扩散模型（例如 Stable Diffusion）的神经网络。它允许输入调节图像，然后使用该调节图像来操控图像生成。可以将原始图像视为空间条件控制，在InstructPix2Pix数据集上训练ControlNet，以实现指令驱动的图像编辑，称为ControlNet-InstructPix2Pix。
（3） 改进InstructPix2Pix
HIVE 通过利用人类反馈来解决编辑指令和生成的编辑图像之间的不一致问题，从而改进了 InstructPix2Pix。MagicBrush 引入了第一个大规模的手动注释数据集，用于指令引导的真实图像编辑，并在数据集上微调InstructPix2Pix以获得更好的性能。MGIE使用多模态大语言模型来导出表达指令并提供显式指导，在保持效率的同时进一步提高了编辑性能。
此外，还有一些流行的文本驱动的图像编辑方法，例如 Inpainting 和 DDIM Inversion 。Inpainting 通过屏蔽图像的某些区域来编辑图像，然后根据给定的文本重新生成图像。 DDIM Inversion 以原始文本标题为条件执行反向 DDIM 采样过程，将图像转换为其部分噪声版本。然后，通过给定编辑后的文本标题对噪声图像进行去噪来获取编辑后的图像。</description>
    </item>
    
    <item>
      <title>Guided-Diffusion代码理解</title>
      <link>https://Advance2020.github.io/posts/study/guided-diffusion%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://Advance2020.github.io/posts/study/guided-diffusion%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</guid>
      <description>本文所分析的DDPM论文以及代码的相关信息如下:
Diffusion Models Beat GANs on Image Synthesis https://github.com/openai/guided-diffusio (code) 一、算法流程 论文核心方法： 在扩散模型上作出两个方面的改进提升，即改进模型架构(Architecture Improvements)和使用分类器引导（Classifier Guidance），从而在FID上获得优于GAN的表现。
1.通过设计一系列的消融实验，比较各部分改进对FID的影响，最终确定使用架构如下： 2.使用分类器引导的DDPM和DDIM的算法流程分别如下： 二、 代码分析 2.1 分类引导采样流程 采样步骤包括：
加载UNet模型 (预测噪声$\theta$) 和扩散模型 ($\mu_\theta(x_t),\sigma_\theta(x_t)$)； 加载预训练好的分类噪声图像的分类器 $p_\phi(y|x_t)$； 进行DDPM或DDIM采样过程，并加入引导梯度； 将样本转化为图片并保存。 相关核心函数调用见下图（以DDPM采样为例）： classifier_sample.py代码如下：
1#使用一个噪声图像分类器来引导采样过程，从而生成更逼真的图像 2#非核心代码已省略.... 3def main(): 4 logger.log(&amp;#34;creating model and diffusion...&amp;#34;) 5 model, diffusion = create_model_and_diffusion(#初始化UNet模型和扩散模型 6 **args_to_dict(args, model_and_diffusion_defaults().keys())) 7 model.load_state_dict( 8 dist_util.load_state_dict(args.model_path, map_location=&amp;#34;cpu&amp;#34;)) 9 model.to(dist_util.dev()) 10 if args.use_fp16: 11 model.convert_to_fp16()#使用浮点进行原始模型的训练推理，float16加快速度 12 model.eval()#.train()模式主要用于激活某些特定于训练的层如Dropout和BatchNorm） 13 # 而.eval()模式则确保这些层在评估或测试时不激活 14 logger.log(&amp;#34;loading classifier...&amp;#34;) 15 classifier = create_classifier(**args_to_dict(args, classifier_defaults().</description>
    </item>
    
    <item>
      <title>DDPM基础代码解析</title>
      <link>https://Advance2020.github.io/posts/study/ddpm%E5%9F%BA%E7%A1%80%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Sun, 03 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://Advance2020.github.io/posts/study/ddpm%E5%9F%BA%E7%A1%80%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</guid>
      <description>本文所分析的DDPM论文以及代码的相关信息如下:
Denoising Diffusion Probabilistic Models (论文) 直接调用预训练好的扩散模型demo版（Pytorch） 一、 demo.py 首先由__init__初始化一些显示界面的参数，如进度条。 然后进入main()函数，在main()函数中完成了以下任务:
数据集、ema多选框、扩散步骤的预设置 get_state()实例化扩散过程，包括加载预训练的扩散模型、$x_T$、以及设定扩散步骤 实时显示当前图像 $x_t$ 和 cur_step 根据页面触发的动作Denoise和Diffuse来分别调用去噪函数和扩散函数，如下图： 1def main(): 2 #无关代码已省略 ...... 3 name = st.sidebar.radio(&amp;#34;Model&amp;#34;, (&amp;#34;cifar10&amp;#34;, &amp;#34;lsun_bedroom&amp;#34;, &amp;#34;lsun_cat&amp;#34;, &amp;#34;lsun_church&amp;#34;)) 4 ema = st.sidebar.checkbox(&amp;#34;ema&amp;#34;, value=True) 5 state = get_state(name, ema=ema) 6 diffusion = state[&amp;#34;diffusion&amp;#34;] 7 #&amp;#34;Number of steps&amp;#34; 默认为1000 8 n_steps = st.sidebar.number_input(&amp;#34;Number of steps&amp;#34;, min_value=1,max_value=diffusion.num_timesteps,value=diffusion.num_timesteps) 9 def callback(x, i, x0=None): 10 if show_x0 and x0 is not None: 11 x = x0 12 output.</description>
    </item>
    
    <item>
      <title>DDPM代码详解</title>
      <link>https://Advance2020.github.io/posts/study/ddpm%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://Advance2020.github.io/posts/study/ddpm%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3/</guid>
      <description>1.算法流程 DDPM的算法流程如下： 训练阶段重复以下步骤：
从数据集中采样$x_0$ 随机选取 time step $t$ 生成高斯噪声 $\epsilon \in \mathcal N(0,I)$ 调用模型预估 $\epsilon_{\theta}(\sqrt{\bar{\alpha _t}}x_0+\sqrt{(1-\bar{\alpha_t})}\epsilon_t,t)$ 计算噪声之间的 MSE loss: $\parallel \epsilon_t-\epsilon_{\theta}(\sqrt{\bar \alpha_t}x_0+\sqrt{(1-\bar\alpha_t)}\epsilon_t,t)\parallel^2$ 逆向阶段采样如下步骤进行采样：
从高斯分布采样$x_T$ 按照 $T,&amp;hellip;,1$ 的顺序进行迭代： 如果 $t=1$,令$z=0$; 如果 $t&amp;gt;1$, 从高斯分布中采样 $z\sim \mathcal{N}(0,I)$ 学习出均值 $\mu_\theta(x_t,t)=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_\theta(x_t,t))$, 学习方差 $\sigma_t=\sqrt{\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\cdot\beta_t}$ , 即 $\sqrt{\tilde{\beta}_t}$ 通过重参数技巧采样 $x_{t-1}=\mu_\theta(x_t,t)+\sigma_tz$ 经过以上过程的迭代，最终恢复 $x_0$ . 2.源码分析 DDPM 文章以及代码的相关信息如下:
Denoising Diffusion Probabilistic Models 论文 本文分析的Pytorch源码:https://github.com/tqch/ddpm-torch 训练阶段
以cifar10数据集为例，在 train.py 中进行前向传播计算Loss:
1def train(self, evaluator=None, chkpt_path=None, image_dir=None): 2 #无关代码已省略 3 #...... 4 global_steps = 0 5 for e in range(self.</description>
    </item>
    
    <item>
      <title>深度学习基础笔记</title>
      <link>https://Advance2020.github.io/posts/study/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://Advance2020.github.io/posts/study/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/</guid>
      <description>一、理解variance 与 bias varivance：反映准确率 bias:反映模型的拟合能力
高方差（variance）—— 过拟合 —— 训练集上表现好，验证集上表现差 高偏差（bias）—— 欠拟合 —— 训练集上表现就比较差 解决方法：对于high bias，增加模型复杂度，训练一个更大的网络；对于high variance，增加训练数据；采用正则化的方法
二、NCHW与NHWC数据格式 （1） 基本概念
深度学习框架中，数据一般是4D，用NCHW或NHWC表达，其中：
N - Batch C - Channel H - Height W - Width （2）逻辑表达
假定N = 2，C = 16，H = 5，W = 4，那么这个4D数据，看起来是这样的： 我们比较直接的理解方式是3D，上图中从三个方向上理解，C方向/H方向/W方向。再加N方向上，就是4D。上图中红色标准的数值是这个数据里每个元素的数值。
（3）逻辑表达
无论逻辑表达上是几维的数据，在计算机中存储时都是按照1D来存储的。NCHW和NHWC格式数据的存储形式如下图所示： NCHW是先取W方向数据；然后H方向；再C方向；最后N方向。 NHWC是先取C方向数据；然后W方向；再H方向；最后N方向。 （4）不同框架支持
目前的主流ML框架对NCHW和NHWC数据格式做了支持，有些框架可以支持两种且用户未做设置时有一个缺省值:
TensorFlow：缺省NHWC，GPU也支持NCHW Caffe：NCHW PyTorch：NCHW 三、小数化整数 math.round() —— 四舍五入，注意正负数的不同 math.ceil() —— 向上取整，即小数部分直接舍去，并向正数部分进1 math.floor() —— “向下取整” ，即小数部分直接舍去 四、lambda函数 （1）lambda函数语法
表现形式如下：lambda [arg1,[arg2,.....argn]]:expression
其中，lambda 是 Python 预留的关键字，[arg…] 和 expression 由用户自定义。 （2）lambda函数特性</description>
    </item>
    
    <item>
      <title>DDIM论文理解</title>
      <link>https://Advance2020.github.io/posts/study/ddim%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3/</link>
      <pubDate>Fri, 13 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://Advance2020.github.io/posts/study/ddim%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3/</guid>
      <description>一、论文简介 DENOISING DIFFUSION IMPLICIT MODELS
会议来源：ICLR 文章年限：2021
核心思路：在DDPM(DENOISING DIFFUSION probabilistic MODEL)的启发下，用非马尔可夫过程改变生成过程，不用一步一步去噪，从而能够更快地产生高质量样本。
二、论文理解 1. 背景与目的 已经有多种模型都在图片生成上展示出了或多或少的优势。其中GAN比基于似然的模型如变分自动编码器、自回归模型表现出更高的样本质量。但是同时，GAN也具有一些不可忽视的缺点：训练过程不稳定；在某些情况下，GAN可能会遇到模式崩溃，导致生成的样本质量下降。
针对这一不足，基于马尔可夫链的迭代生成模型应运而生，如DDPM和噪声条件评分模型，它们不需要对抗训练，也同样可以生成较好质量的样本。但是同样也存在一个关键的缺点——迭代次数多，生成样本的速度慢。
为了提高采样速度，在DDPM的启发下，在损失目标函数相同的情况下，作出了相应的改进，提出了DDIM。DDIM相较于DDPM的优点：
在保证提高10到50倍的采样速度的同时，有更高的生成质量 有一致性属性——在初始潜在变量相似的情况下，由不同长度的马尔可夫链生成的样本具有相似的高层特征。 利用“一致性”，可以通过初始潜在变量来进行在语义上有意义的图像插值 2. 核心方法 以生成过程为切入点，一个关键的观察结果是，Lγ 形式的 DDPM 目标仅取决于边际值q(xt|x0)，而不直接取决于联合 q(x1:T |x0)。由此得到启发：DDPM这个隐变量模型可以有很多推理分布来选择，只要推理分布满足边缘分布条件（扩散过程的特性）即可，而且这些推理过程并不一定要是马尔卡夫链。 由于有许多具有相同边际的推理分布，因此可以找出非马尔可夫的替代推理过程，从而产生新的生成过程（图 1，右）。这些非马尔可夫推理过程产生与 DDPM 相同的代理目标函数。 2.1 非马尔可夫链的前向过程 推理分布族 Q： 其中 $q_\sigma(x_T|x_0) = \mathcal{N}(\sqrt{\alpha_T}x_0,(1-\alpha_T)I)$ 且 $t&amp;gt;1$， 由贝叶斯公式可以得出： 这里每个 xt 都可以依赖于 xt−1 和 x0，因此前向过程不再是马尔可夫的。 当 σ → 0 时，达到了一种极端情况，只要在某个 t 内观察 x0 和 xt，那么 xt−1 就已知并固定。（！！！公式代入推理）
2.2 生成过程和统一变分推理目标 （1）生成过程pθ (x0:T )：每一步的去噪都是从过程qσ(xt−1|xt, x0)中学习，σ 的大小控制着前向过程的随机性；
先由给定的xt 预测出x0(见公式9) 然后得到生成过程 通过样本 xt 生成样本 xt−1： 通过改变σ 的可以改变生成过程，当$σ_t = \sqrt{(1 − α_{t−1})/(1 − α_t)}\sqrt{1 − α_t/α_{t−1}}$时，就是马尔可夫链的DDPM。（！！！公式推理）</description>
    </item>
    
    <item>
      <title>博客更新&amp;&amp;Git认证</title>
      <link>https://Advance2020.github.io/posts/tips/%E5%8D%9A%E5%AE%A2%E6%9B%B4%E6%96%B0git%E8%AE%A4%E8%AF%81/</link>
      <pubDate>Wed, 11 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://Advance2020.github.io/posts/tips/%E5%8D%9A%E5%AE%A2%E6%9B%B4%E6%96%B0git%E8%AE%A4%E8%AF%81/</guid>
      <description>一、博客更新常用命令 废柴小白刚上手老是记不住哇，好记性不如烂笔头，把常用命令放下面，下次就好找啦^0^ （哼，看我用的多了还记不住你！！！ 首先打开cmd，输入以下命令
1hugo --theme=yinyang --baseURL=&amp;#34;https://Advance2020.github.io&amp;#34; --buildDrafts 然后进入Public文件夹，打开git:
1git add . 2git commit -m &amp;#34;mmmmm&amp;#34; 3git push -u origin matser 二、Git认证 这次push总是提交失败，各种问题（time out、reset、denied）全碰了一遍，真是不疯魔不成活。踩完各种雷，还是想整理整理，说不定下次还要踩。
使用令牌登录 不要使用用户名密码，git已经改了。去github-settings-Developer settings生成Tokens(classic)。划重点，生成令牌一定要勾选各种权限，不然就等着Permission denied!
常用命令 执行这个命令之后，可以重新写入账号令牌 1git config --system --unset credential.helper 配置为缓存密码 之后的操作就不需要输入密码了。 1git config --global credential.helper store 使用下面命令清除使用密码缓存的配置 1git config --local --unset credential.helper 2git config --global --unset credential.helper 3git config --system --unset credential.helper PS: 令牌会过期，到时候重新生成一个就好。</description>
    </item>
    
    <item>
      <title>ML-LOO论文梳理</title>
      <link>https://Advance2020.github.io/posts/study/ml-loo%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/</link>
      <pubDate>Sun, 13 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://Advance2020.github.io/posts/study/ml-loo%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/</guid>
      <description>使用特征归因检测对抗样本 Abstract 通过对特征归因得分的尺度估计进行阈值化进行对抗样本的检测 采取多层特征归因应对具有混合置信度的攻击 在攻击者可以完全访问分类器的情况下依然保持较好的检测效果 1.Introduction 虽然一系列工作试图解释为什么存在对抗样本，但尚未对根本原因进行全面分析，这主要是因为深层原因神经网络具有复杂的函数形式，因此很难获得数学特征。另一方面，特征归因是一种被广泛研究解决神经网络黑盒性质的方法：给定一个预测模型，对每个输入实例输出一个与底层特征相关的重要性得分向量。特征归因已被用来提高机器学习模型的透明度和公平性。
本文研究了特征归因在检测对抗样本中的应用。观察到边界附近的对抗样本的特征归因图总是与相应的原始示例的特征归因图不同。图 1 显示了一个激励示例，它演示了将 CIFAR-10 中的图像输入残差神经网络以及留一法 (LOO) 的相应特征归因。后者通过观察在 C&amp;amp;W 攻击的最坏情况扰动之前和之后擦除输入的每个像素对模型的影响来解释神经模型的决策。虽然原始图像上的扰动在视觉上难以察觉，但特征属性却发生了巨大的变化。进一步观察到，这种差异可以通过描述特征不一致的简单统计来概括，这些统计能够区分对抗样本和自然图像。推测这是因为对抗样本往往会将样本扰乱到决策面上的不稳定区域。
上述观察产生了一种检测决策边界附近的对抗样本的有效方法。另一方面，也存在模型具有高置信度的对抗样本。还观察到一个有趣的现象：即使对于高置信度的对抗样本，神经网络的中间层仍然包含不确定性信息。基于这一观察，将不稳定方法推广到合并多层特征归因，其中计算中间层的归因分数而不会产生额外的模型查询。
2.相关工作 特征归因 已经提出了多种方法来分配特征归因分数。对于应用模型的每个特定实例，归因方法通过实例周围的局部线性模型逼近目标模型，为每个特征分配重要性分数。一类方法假设模型的可微性，并通过梯度将预测传播到特征。包括直接使用梯度（显着性图）、逐层相关性传播（LWRP）及其改进版本 DeepLIFT和积分梯度。
另一类方法是基于扰动的，因此与模型无关。给定一个实例，通过使用预先指定的参考值屏蔽不同的特征组来生成多个扰动样本。实例的特征属性是根据模型对这些样本的预测分数来计算的，包括留一法、LIME和 KernelSHAP。
观察结果表明，特征归因的敏感性可能源于模型的敏感性，而不是归因方法。这激发了本文通过归因方法检测对抗样本。
对抗攻击 对抗攻击试图以最小的扰动改变给定模型对原始实例的预测，从而产生对抗样本。对抗样本可以分为有针对性的或无针对性的，具体取决于目标是将受到干扰的实例分类为给定的目标类还是与正确类不同的任意类。攻击还因用于表征最小扰动的距离类型而异。 ∞、0 和 2 距离是最常用的距离。 Goodfellow、Shlens 和 Szegedy 提出的快速梯度符号法 (FGSM) 是一种最小化 ∞ 距离的有效方法。 Kurakin、Goodfellow、Bengio 和 Madry 等人提出了 ∞-PGD (BIM)，它是 FGSM 的迭代版本，它以更小的扰动实现了更高的成功率。 Moosavi-Dezfooli、Fawzi 和 Frossard 提出的 DeepFool 通过迭代线性化过程最小化 2 距离。 Carlini 和 Wagner 提出了有效的算法来为这三个距离中的每一个生成对抗样本。特别是，Carlini 和 Wagner 提出了一种能够控制对抗样本置信度的损失函数。 (Papernot et al. 2016a) 提出的基于雅可比行列式的显着图攻击 (JSMA) 是一种用于 0 度量扰动的贪婪方法。最近，出现了几种仅依赖于概率分数或决策的黑盒对抗性攻击。 Ilyas、Engstrom 和 Madry 引入了基于分数的方法，使用零阶梯度估计来制作对抗样本。 Brendel、Rauber 和 Bethge 引入了边界攻击，作为一种最小化 2 距离的黑盒方法，不需要访问梯度信息，仅依赖于模型决策。</description>
    </item>
    
    <item>
      <title>凌晨三点的指尖</title>
      <link>https://Advance2020.github.io/posts/random/cn/</link>
      <pubDate>Wed, 09 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://Advance2020.github.io/posts/random/cn/</guid>
      <description>梦魇缠身 vs 惨遭误伤 凌晨一点追完品质好剧《做自己的光》大结局，我俩才慢悠悠躺床上，却都毫无睡意。玩闹了半晌，才终于舍得放下手机，做出入睡的架势。奈何29℃的空调如何是懂让人浑身湿答答的，两人轮流下床调了几回，终于差强人意，才开始用此起彼伏舒缓的鼾声来宣告夜晚的静谧。
熟睡着，忽然胸口一阵疼痛，猛地睁眼，看见身旁的半个人影梦呓着坐将起来，一个支撑个大半个身子的手肘就这样压着我的胸口不断往下坠，疼的我啊啊地叫出声来。
“我做噩梦了😫” 身旁人梦魇中惊醒，余悸仍未消，开始颤抖起来。我疼的不想呼吸，又想到莫名挨痛，一肚子闷气。再加上白天又喝了一杯瑞幸，脑袋是嘎嘎清醒了，睡不着，翻来覆去了一会儿，下床继续弄起了下午未竣工的宝贝网站。
修改这个配置，撰写那个文档，指尖在清脆的伴奏下翩然起舞，这方小小天地终于闪露出了星微光亮，指尖却舞得更加热烈，迫不及待地完成了首篇“文章”，满足！回去补补觉，又是进步的一天呐！
PS: 我寻思着早就想给某人准备的捕梦网应该要提上日程了，把扰人的恶梦通通困在网中，并随着次日的阳光灰飞烟灭，消失得无影无踪……</description>
    </item>
    
  </channel>
</rss>
